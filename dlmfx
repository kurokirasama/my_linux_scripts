#!/usr/bin/bash

cd ~/manga
url=$1

temp=${url%/*}
baseurl=$temp'/'
manga=$(echo $url | tr '/' '\n' | sed '5q;d')
chapter=$(basename $temp)
dirname=$manga'_'$chapter

echo creating directory $dirname...
mkdir -p $dirname
cd $dirname

pagenumber=1
nextpage=$baseurl$pagenumber'.html'
links=''

curl --connect-timeout 5 --max-time 10 --retry-max-time 40 --retry 5 --retry-delay 0 -s --compressed -o page.html $nextpage

echo getting pages links...
while [ $(stat -c '%s' page.html) -gt 0 ]; do
	
	imglink=$(grep "compressed" page.html | grep "width" | tr '"' '\n' | grep "compressed")
	
	if [ ! "$imglink" ]; then
		nextpage=$baseurl'0'$pagenumber'.html'
		
		curl --connect-timeout 5 --max-time 10 --retry-max-time 40 --retry 5 --retry-delay 0 -s --compressed -o page.html $nextpage
		
		imglink=$(grep "compressed" page.html | grep "width" | tr '"' '\n' | grep "compressed")
	fi
		
	filename=$(basename $imglink)
	filename=${filename%\?*}
	
	echo page $pagenumber:$filename
	
	links=$links$filename' '$imglink'\n'
	
	let pagenumber=pagenumber+1
	nextpage=$baseurl$pagenumber'.html'
	#curl --connect-timeout 5 --max-time 10 --retry-max-time 40 --retry 5 --retry-delay 0 -s -k -f -C - -o $filename $imglink

	curl --connect-timeout 5 --max-time 10 --retry-max-time 40 --retry 5 --retry-delay 0 -s --compressed -o page.html $nextpage
done

links=${links::-2}
rm page.html
#echo -e $links > links.txt
echo downloading pages...
echo -e $links | parallel --colsep " " curl --connect-timeout 5 --max-time 10 --retry-max-time 40 --retry 5 --retry-delay 0 -s -k -f -C - -o '{1}' '{2}'

if [ $(ls -l | wc -l) -lt $(echo -e $links | wc -l) ]; then
	echo some pages are missing, try again
	termux-notification --title "some pages are missing, try again" --vibrate 500,1000 --content "" --priority high --led-on 800
	exit
fi

echo compressing $dirname...
cd ..
7z a -bso0 -bsp0 -sdel $dirname.7z $dirname
title=$dirname' downloaded'
termux-notification --title "$title" --vibrate 500,1000 --content "" --priority high --led-on 800